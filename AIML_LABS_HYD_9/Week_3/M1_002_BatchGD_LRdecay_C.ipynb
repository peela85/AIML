{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M1_002_BatchGD_LRdecay_C.ipynb","version":"0.3.2","provenance":[{"file_id":"1ga88fRoXmt9BRzgWvgj2kWoMoLNrBUMm","timestamp":1548224580383}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"tao2w0W4IwEO","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"metadata":{"id":"2Pkm2A7Ss0g1","colab_type":"text"},"cell_type":"markdown","source":["### Learning Objective"]},{"metadata":{"id":"YEjLydYUgjDV","colab_type":"text"},"cell_type":"markdown","source":["At the end of the experiment, you will be able to :\n","\n","- Understand the concept of Gradient descent method\n","- Observe the effect of learning rate"]},{"metadata":{"id":"XDbRvIzDp_K8","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Experiment Walkthrough\n","#@markdown Gradient descent and learning rate variations\n","from IPython.display import HTML\n","HTML(\"\"\"<video width='520' height='240' controls>\n","<source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Week_1/gradient_descent.mp4\" type='video/mp4'>\n","</video>\"\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"og1EHpbjgrHP","colab_type":"text"},"cell_type":"markdown","source":["##Dataset"]},{"metadata":{"id":"rSwgCEA_uJ40","colab_type":"text"},"cell_type":"markdown","source":["###Description"]},{"metadata":{"id":"sX79m6OkgtKk","colab_type":"text"},"cell_type":"markdown","source":["\n","The dataset consists of two columns and 89 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is  L∝T2 .\n"]},{"metadata":{"id":"nn9drxhC3f6e","colab_type":"text"},"cell_type":"markdown","source":["##AI/ML Technique"]},{"metadata":{"id":"M00g7i4RvHN7","colab_type":"text"},"cell_type":"markdown","source":["#### Gradient Descent\n","\n","Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on a convex function, that tweaks it’s parameters iteratively to minimize a given function to its local minimum.\n","\n","To know more about Gradient Descent and its variants you can refer the below link:\n","\n","https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0"]},{"metadata":{"id":"A1HXAgaNuvZn","colab_type":"text"},"cell_type":"markdown","source":["## Keywords"]},{"metadata":{"id":"F6HD2LjdIwEQ","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","- Gradient Descent\n","- Learning Rate\n","- Error Function\n","- Decay "]},{"metadata":{"id":"a3fJnELiIwES","colab_type":"text"},"cell_type":"markdown","source":["## Expected time : 60 mins"]},{"metadata":{"id":"j1b476ZK6U74","colab_type":"text"},"cell_type":"markdown","source":["## Setup Steps"]},{"metadata":{"id":"AQylO_MO6SUK","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GnQgepji6Zaa","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F7oXwE9nv30Y","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook=\"M1W1_002_BatchGD_LRdecay_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\") \n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Exp1/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", r[\"date\"])\n","      print(\"Time of submission: \", r[\"time\"])\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","    from IPython.display import HTML\n","    HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id))\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HxcZUqq5IwEY","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import the required Packages\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Ttnpu44IwEb","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load the data by using pandas read_csv()\n","data = pd.read_csv(\"AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n","# Print the first 5 rows of dataframe 'data'\n","print(data.head())\n","# Print the last 5 rows of dataframe 'data'\n","print(data.tail())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZGdRFp1LIwEd","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get the length and time period values from the dataset\n","l = data['l'].values\n","t = data['t'].values\n","# Get the square of time period\n","tsq = t * t"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pUL8e-6JIwEg","colab_type":"text"},"cell_type":"markdown","source":["#### Vanilla/Batch Gradient Descent"]},{"metadata":{"id":"8m6BE6v-IwEg","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","The function 'train' updates the values of m and c and calculates error. \n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized error is returned.\n","\"\"\"\n","def train(x, y, m, c, eta):\n","    const = - 2.0/len(y)\n","    ycalc = m * x + c\n","    #Calculate \"delta_m\" and \"delta_c\"\n","    #<YOUR CODE HERE>\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    error = sum((y - ycalc)**2)/len(y)\n","    return m, c, error\n","\n","\"\"\"\n","The function below trains the data for 1000 iterations. \n","In each iteration it calls the 'train' function to get the updated values of m, c and error.\n","\"\"\"\n","def train_on_all(x, y, m, c, eta, iterations=1000):\n","    for steps in range(iterations):\n","        m, c, err = train(x, y, m, c, eta)\n","    return m, c, err"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MHjYX45BIwEj","colab_type":"text"},"cell_type":"markdown","source":["### Effect of varying LR on error and final line\n","\n","Let us vary LR and find how the error decreases in each case, and how the final line looks, by training each case for the same number of iterations - 2000."]},{"metadata":{"id":"XydFY0FzIwEk","colab_type":"text"},"cell_type":"markdown","source":["### $\\eta$ = 0.1, 0.01, 0.001 and 0.0001 each for 2000 iterations figure out values of m, c. These values (i.e. m_1, m_01 etc c_1, c_01 etc) will be used in the cell below to plot."]},{"metadata":{"id":"295Ou0i68VVU","colab_type":"code","colab":{}},"cell_type":"code","source":["#<YOUR CODE HERE>"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QNZgQMNZIwE0","colab_type":"text"},"cell_type":"markdown","source":["## Plot of lines vs $\\eta$"]},{"metadata":{"id":"XWFue0e6IwE1","colab_type":"code","colab":{}},"cell_type":"code","source":["# Find the lines\n","y_1 = m_1 * l + c_1\n","y_01 = m_01 * l + c_01\n","y_001 = m_001 * l + c_001\n","y_0001 = m_0001 * l + c_0001"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LyCs_U80IwE4","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(15, 8))\n","#For all the lines above i.e. y_1, y_01, y_001, y_0001 plot them (in a different color) against the lines \"l\" \n","#<YOUR CODE HERE>\n","plt.legend([\"l vs tsq\",\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oWp-qhiZIwE6","colab_type":"text"},"cell_type":"markdown","source":["Thus, we see that higher learning rates reach the best fit faster than lower learning rates (obviously)."]},{"metadata":{"id":"GNyPltb7IwE7","colab_type":"text"},"cell_type":"markdown","source":["## Plot of errors vs epochs for each $\\eta$"]},{"metadata":{"id":"ds8LI8XBIwE7","colab_type":"code","colab":{}},"cell_type":"code","source":["epochs = range(0,2000)\n","plt.figure(figsize=(16,10))\n","#plot \"epochs\" versus errs_1, errs_01, errs_02 (each in a different color)\n","plt.legend([\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MqL3Qd4dIwFA","colab_type":"text"},"cell_type":"markdown","source":["# With LR Decay\n","\n","In some cases, the learning rate might be too high to give good fitting lines. For example, let us train with constant LR of 0.8 and get the final line after 1000 iterations:"]},{"metadata":{"id":"eoW3VvR8IwFB","colab_type":"text"},"cell_type":"markdown","source":["### $\\eta$ = 0.8"]},{"metadata":{"id":"A_Z-use6IwFC","colab_type":"code","colab":{}},"cell_type":"code","source":["errs = []\n","m, c = 0, 0\n","eta = 0.8\n","# Call the train() method for 1000 iterations to update m and c and get error value with constant eta = 0.8.\n","for times in range(1000):\n","    #Call train function below\n","    m, c, error = #<YOUR CODE HERE>\n","    #append to \"errs\" to append the errors for plotting later\n","    #<YOUR CODE HERE>\n","\n","    \n","m_normal, c_normal = m, c"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TjmDAryWIwFG","colab_type":"text"},"cell_type":"markdown","source":["Let us see the plot of error vs iterations:"]},{"metadata":{"id":"s0N0o3NKIwFH","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.plot(range(len(errs)), errs)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yN5_kJn0IwFJ","colab_type":"text"},"cell_type":"markdown","source":["We see that the error quickly goes to almost 0, but after some iterations blows up.\n","\n","Let us check the \"best fit\" line that is found:"]},{"metadata":{"id":"DXumozZ-IwFL","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_normal, c_normal, errs[-1]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5zB7RTTOIwFM","colab_type":"code","colab":{}},"cell_type":"code","source":["y = m_normal * l + c_normal \n","plt.plot(l, tsq, '.k', label = 'Actual')\n","plt.plot(l,y,\"r\", label = 'Prediction')\n","plt.xlabel(\"Length (L)\")\n","plt.ylabel(\"T^2\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fqmvtu-_IwFQ","colab_type":"text"},"cell_type":"markdown","source":["Clearly this is not ideal.\n","\n","This was a simple case where we can see the learning rate is too high. There might be cases where it is not so simple to identify this. Also, having a low learning rate is not good because training time would be too high!\n","\n","**Solution: Decay the learning rate.**\n","\n","Now let us train another model with decaying lr. But let us not decay lr below 0.0001."]},{"metadata":{"id":"edZDbSyBIwFR","colab_type":"code","colab":{}},"cell_type":"code","source":["errs_decay = []\n","m, c = 0, 0\n","eta = 0.5\n","decay_factor = 0.99\n","# Call the train() method for 1000 iterations to update m and c and get error value with decaying eta.\n","for iteration in range(1000):\n","    eta = max(0.0001, eta * decay_factor)\n","    #Call train function below\n","    m, c, error = #<YOUR CODE HERE>\n","    #append to \"errs\" to append the errors for plotting later\n","    #<YOUR CODE HERE>\n","\n","\n","m_decay, c_decay = m, c"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xbEAsf4cIwFS","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_decay, c_decay, errs_decay[-1]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"clqa9StFVKqP","colab_type":"text"},"cell_type":"markdown","source":["Let us see the plot of error vs iterations:"]},{"metadata":{"id":"WNUxY6ZyIwFV","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.plot(range(len(errs_decay)), errs_decay)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fRoeB71EIwFW","colab_type":"code","colab":{}},"cell_type":"code","source":["y = m_decay * l + c_decay \n","plt.plot(l, tsq, '.k')\n","plt.plot(l,y,\"r\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"djZn9yLqIwFa","colab_type":"text"},"cell_type":"markdown","source":["Thus, this is correct."]},{"metadata":{"id":"Go5quY3c66w7","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"f-bZhl8VoIGg","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Gradient descent always finds the global minima? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"True\",\"False\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f6XkwFo0IwFb","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7eWGLStgIwFd","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hcuvkhRq7K-i","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8wBBpKKU7Nte","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]}]}