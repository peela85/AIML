{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKxW4D6DHCyx"
   },
   "source": [
    "# Advanced Certification in AIML\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzkHDYHGZnyC"
   },
   "source": [
    "### Learning Objectives:\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "*  Preprocessing text data\n",
    "*  Representation of  text document using Bag of Words\n",
    "*  Understand Bag of Words represented text data with K-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzskman3ZuWC"
   },
   "source": [
    "### Dataset\n",
    "In this experiment we use the 20 newsgroup dataset\n",
    "\n",
    "**Description**\n",
    "\n",
    "This dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. That is there are approximately one thousand documents taken from each of the following newsgroups:\n",
    "\n",
    "    alt.athesim\n",
    "    comp.graphics   \n",
    "    comp.os.ms-windows.misc\n",
    "    comp.sys.ibm.pc.hardware\n",
    "    comp.sys.mac.hardware\n",
    "    comp.windows.x\n",
    "    misc.forsale\n",
    "    rec.autos\n",
    "    rec.motorcycles\n",
    "    rec.sport.baseball\n",
    "    rec.sport.hockey\n",
    "    sci.crypt\n",
    "    sci.electronics\n",
    "    sci.med\n",
    "    sci.space\n",
    "    soc.religion.christian\n",
    "    talk.politics.guns\n",
    "    talk.politics.mideast\n",
    "    talk.politics.misc\n",
    "    talk.religion.misc\n",
    "\n",
    "The dataset consists **Usenet** posts--essentially an email sent by subscribers to that newsgroup. They typically contain quotes from previous posts as well as cross posts i.e. a few posts may be sent to more than once in a newsgroup.\n",
    "\n",
    "Each newsgroup is stored in a subdirectory, with each post stored as a separate file.\n",
    "\n",
    "Data source to this experiment : http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vW1Vu2adZ0oO"
   },
   "source": [
    "### Domain Information\n",
    "A newsgroup, despite the name, has nothing to do with news. It is what we would call today a mailing list or a discussion forum. *Usenet* is a distributed discussion system designed and developed in 1979 and deployed in 1980.  \n",
    "\n",
    "Members joined newsgroups of interest to them and made *posts* to them. Posts are very similar to email -- in later years, newsgroups became mailing lists and people posted via email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bo3iVdSkZ5gb"
   },
   "source": [
    "The problem that we are attempting is \"Text classification\". This is a broadly defined task which is common to many services and products: for example, gmail classifies an incoming mail into different sections such as Updates, Forums etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0muqcxoZ6fL"
   },
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "* The bag-of-words is a simple to understand representation of documents and words. As you are aware it makes use of the one-hot representation of each word based on the vocabulary and the document is represented as a sum of the BoW vectors of all the words in the document\n",
    " \n",
    "#### Challenges\n",
    "\n",
    "* The dimension of each vector representing a word is the number of words in the vocabulary. So we definitely will encounter the *curse of dimensionality* \n",
    "* Bag of words representation doesnâ€™t consider the semantic relation between words. \n",
    "* Nor does it capture the grammar of the language--parts of speech etc., "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfgFJFcCHCy2"
   },
   "source": [
    "#### Keywords\n",
    "\n",
    "* Numpy\n",
    "* Collections\n",
    "* Gensim\n",
    "* Bag-of-Words (Word Frequency, Pre-Processing)\n",
    "* Bag-of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xw5JkrNeHCy3"
   },
   "source": [
    "#### Expected Time : 60 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YuhepSBE20co"
   },
   "source": [
    "### Setup Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbVzyY453Cq2"
   },
   "outputs": [],
   "source": [
    "#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n",
    "Id = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb8Mb_Qc3FUU"
   },
   "outputs": [],
   "source": [
    "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
    "password = \"\" #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1466,
     "status": "ok",
     "timestamp": 1556103301816,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "",
      "userId": "14042875339399760993"
     },
     "user_tz": -330
    },
    "id": "_dRyZo_orf38",
    "outputId": "5f88c200-d8e7-4e59-d0ca-7710adb0c7d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong password. Please try again\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to complete the setup for this Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "  \n",
    "notebook=\"M1W1_001_BOW_20newsgroup_B\" #name of the notebook\n",
    "\n",
    "def setup():\n",
    "#  ipython.magic(\"sx pip3 install torch\")\n",
    "    ipython.magic(\"sx pip3 install gensim\")\n",
    "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_NEWSGROUPS_PICKELFILE.pkl\") \n",
    "    from IPython.display import HTML, display\n",
    "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
    "    print(\"Setup completed successfully\")\n",
    "    return\n",
    "\n",
    "def submit_notebook():\n",
    "    \n",
    "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
    "    \n",
    "    import requests, json, base64, datetime\n",
    "\n",
    "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
    "    if not submission_id:\n",
    "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
    "      r = requests.post(url, data = data)\n",
    "      r = json.loads(r.text)\n",
    "\n",
    "      if r[\"status\"] == \"Success\":\n",
    "          return r[\"record_id\"]\n",
    "      elif \"err\" in r:        \n",
    "        print(r[\"err\"])\n",
    "        return None        \n",
    "      else:\n",
    "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
    "        return None\n",
    "\n",
    "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n",
    "      f = open(notebook + \".ipynb\", \"rb\")\n",
    "      file_hash = base64.b64encode(f.read())\n",
    "\n",
    "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
    "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
    "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
    "              \"notebook\" : notebook}\n",
    "\n",
    "      r = requests.post(url, data = data)\n",
    "      r = json.loads(r.text)\n",
    "      print(\"Your submission is successful.\")\n",
    "      print(\"Ref Id:\", submission_id)\n",
    "      print(\"Date of submission: \", r[\"date\"])\n",
    "      print(\"Time of submission: \", r[\"time\"])\n",
    "      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n",
    "      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
    "      return submission_id\n",
    "    else: submission_id\n",
    "    \n",
    "\n",
    "def getAdditional():\n",
    "  try:\n",
    "    if Additional: return Additional      \n",
    "    else: raise NameError('')\n",
    "  except NameError:\n",
    "    print (\"Please answer Additional Question\")\n",
    "    return None\n",
    "\n",
    "def getComplexity():\n",
    "  try:\n",
    "    return Complexity\n",
    "  except NameError:\n",
    "    print (\"Please answer Complexity Question\")\n",
    "    return None\n",
    "  \n",
    "def getConcepts():\n",
    "  try:\n",
    "    return Concepts\n",
    "  except NameError:\n",
    "    print (\"Please answer Concepts Question\")\n",
    "    return None\n",
    "\n",
    "def getAnswer():\n",
    "  try:\n",
    "    return Answer\n",
    "  except NameError:\n",
    "    print (\"Please answer Question\")\n",
    "    return None\n",
    "\n",
    "def getId():\n",
    "  try: \n",
    "    return Id if Id else None\n",
    "  except NameError:\n",
    "    return None\n",
    "\n",
    "def getPassword():\n",
    "  try:\n",
    "    return password if password else None\n",
    "  except NameError:\n",
    "    return None\n",
    "\n",
    "submission_id = None\n",
    "### Setup \n",
    "if getPassword() and getId():\n",
    "  submission_id = submit_notebook()\n",
    "  if submission_id:\n",
    "    setup()\n",
    "    from IPython.display import HTML\n",
    "    HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id))\n",
    "  \n",
    "else:\n",
    "  print (\"Please complete Id and Password cells before running setup\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2676,
     "status": "ok",
     "timestamp": 1556103059826,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "",
      "userId": "14042875339399760993"
     },
     "user_tz": -330
    },
    "id": "K8-lZrNaHCy8",
    "outputId": "8fd11741-b4fc-4985-b6d1-f1894de3eb90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# Importing required Packages\n",
    "import pickle\n",
    "import re\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iy0iuGsTHCzA"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = pickle.load(open('AIML_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n",
    "print(type(dataset))\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2k1kcF_HCy0"
   },
   "source": [
    "To get a sense of our data, let us first start by counting the frequencies of the target classes in our news articles in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8T5v8vGHCzC"
   },
   "outputs": [],
   "source": [
    "# Print frequencies of dataset\n",
    "print(\"Class : count\")\n",
    "print(\"--------------\")\n",
    "number_of_documents = 0\n",
    "for key in dataset:\n",
    "    print(key, ':', len(dataset[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cl74CoG8HCzE"
   },
   "source": [
    "Next, let us split our dataset which consists of  about 1000 samples per class, into training and test sets. We use about 95% samples from each class in the training set, and the remaining  in the test set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgUy5WyFHCzF"
   },
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "test_set = {}\n",
    "new_dataset = {}\n",
    "\n",
    "# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n",
    "for key in dataset:\n",
    "    new_dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n",
    "    \n",
    "# Break dataset into 95-5 split for training and testing\n",
    "n_train = 0\n",
    "n_test = 0\n",
    "for k in new_dataset:\n",
    "    split = int(0.95*len(new_dataset[k]))\n",
    "    train_set[k] = new_dataset[k][0:split]\n",
    "    test_set[k] = new_dataset[k][split:]\n",
    "    n_train += len(train_set[k])\n",
    "    n_test += len(test_set[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvwvqYpJHCzH"
   },
   "source": [
    "## 1. Bag-of-Words\n",
    "\n",
    "Let us begin our journey into text classification with one of the simplest but most commonly used feature representations for news documents - Bag-of-Words.\n",
    "\n",
    "As you might have realized, machine learning algorithms need good feature representations of different inputs.  Concretely, we would like to represent each news article $D$ in terms of a feature vector $V$, which can be used for classification. Feature vector $V$ is made up of the number of occurences of each word in the vocabulary.\n",
    "\n",
    "Let us begin by counting the number of occurences of every word in the news documents in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcoPXdxBHCzI"
   },
   "source": [
    "### 1.1 Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuHlXAyFHCzJ"
   },
   "source": [
    "Let us try understanding the kind of words that appear frequently, and those that occur rarely. We now count the frequencies of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXR_X4GSHCzK"
   },
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store frequencies of words.\n",
    "# Key:Value === Word:Count\n",
    "frequency = defaultdict(int)\n",
    "    \n",
    "for key in train_set:\n",
    "    for f in train_set[key]:\n",
    "        \n",
    "        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n",
    "        # We ignore all special characters such as !.$ and words containing numbers\n",
    "        words = #<YOUR CODE HERE>\n",
    "    \n",
    "        for word in words:\n",
    "            frequency[word] += 1\n",
    "\n",
    "sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Top-10 most frequent words:\")\n",
    "for word in sorted_words[:10]:\n",
    "    print(word)\n",
    "\n",
    "print('----------------------------')\n",
    "print(\"10 least frequent words:\")\n",
    "for word in sorted_words[-10:]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUCZWOwhHCzP"
   },
   "source": [
    "Next, we attempt to plot a histogram of the counts of various words in descending order. \n",
    "\n",
    "Could you comment about the relationship between the frequency of the most frequent word to the second frequent word? \n",
    "And what about the third most frequent word?\n",
    "\n",
    "(Hint - Check the relative frequencies of the first, second and third most frequent words)\n",
    "\n",
    "(After answering, you can visit https://en.wikipedia.org/wiki/Zipf%27s_law for further Reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KV-PAkMBHCzQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,10)\n",
    "#Plot a bar chart of 100 sorted_words, with plotting the words on x-axis and the frequencies on y-axis.\n",
    "#<YOUR CODE HERE>\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZDlJ5cyHCzT"
   },
   "source": [
    "### 1.2 Pre-processing to remove most and least frequent words\n",
    "\n",
    "We can see that different words appear with different frequencies.\n",
    "\n",
    "The most common words appear in almost all documents. Hence, for a classification task, having information about those words' frequencies does not mater much since they appear frequently in every type of document. To get a good feature representation, we eliminate them since they do not add too much value.\n",
    "\n",
    "Additionally, notice how the least frequent words appear so rarely that they might not be useful either.\n",
    "\n",
    "Let us pre-process our news articles now to remove the most frequent and least frequent words by thresholding their counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSPqj3pZHCzT"
   },
   "outputs": [],
   "source": [
    "valid_words = defaultdict(int)\n",
    "\n",
    "print('Number of words before preprocessing:', len(sorted_words))\n",
    "\n",
    "# Ignore the 25 most frequent words, and the words which appear less than 100 times\n",
    "#<YOUR CODE HERE .Populate the valid_words variable as per description above>\n",
    "        \n",
    "print('Number of words after preprocessing:', len(valid_words))\n",
    "\n",
    "word_vector_size = len(valid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEggZhtmHCzV"
   },
   "source": [
    "### 1.3 Bag-of-Words representation\n",
    "\n",
    "The simplest way to represent a document $D$ as a vector $V$ would be to now count the relevant words in the document. \n",
    "\n",
    "For each document, make a vector of the count of each of the words in the vocabulary (excluding the words removed in the previous step - the \"stopwords\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJKDjIjJHCzV"
   },
   "outputs": [],
   "source": [
    "def convert_to_BoW(dataset, number_of_documents):\n",
    "    bow_representation = np.zeros((number_of_documents, word_vector_size))\n",
    "    labels = np.zeros((number_of_documents, 1))\n",
    "    \n",
    "    i = 0\n",
    "    for label, class_name in enumerate(dataset):\n",
    "        \n",
    "        # For each file\n",
    "        for f in dataset[class_name]:\n",
    "            \n",
    "            # Read all text in file\n",
    "            text = ' '.join(f).split(' ')\n",
    "            \n",
    "            # For each word\n",
    "            for word in text:\n",
    "                if word in valid_words:\n",
    "                    bow_representation[i, valid_words[word]] += 1\n",
    "            \n",
    "            # Label of document\n",
    "            labels[i] = label\n",
    "            \n",
    "            # Increment document counter\n",
    "            i += 1\n",
    "    \n",
    "    return bow_representation, labels\n",
    "\n",
    "# Convert the dataset into their bag of words representation treating train and test separately\n",
    "train_bow_set, train_bow_labels = convert_to_BoW(train_set, n_train)\n",
    "test_bow_set, test_bow_labels = convert_to_BoW(test_set, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4bTBjw_HCzX"
   },
   "source": [
    "### 1.4 Document classification using Bag-of-Words\n",
    "\n",
    "For the test documents, use your favorite distance metric (Cosine, Eucilidean, etc.) to find similar news articles from your training set and classify using kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoBjz1NSDJME"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Define and fit the KNeighborsClassifiers\n",
    "#<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbGnpvY6HCzc"
   },
   "source": [
    "Computing accuracy for the bag-of-words features on the full test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CaiubSSDZoI"
   },
   "outputs": [],
   "source": [
    "model.score(test_bow_set, test_bow_labels) # This cell may take some time to finish its execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dv58hpDAHCzk"
   },
   "source": [
    "### Ungraded Exercise 1\n",
    "\n",
    "The frequency thresholds represents the minimum frequency a word must have to be considered relevant. Experiment with the following values of frequency threshold in your preprocessing step from section 1.2. Re-run all the codes with the new set of valid words and check your accuracies. Use the following values:\n",
    "\n",
    "`freq_thresh` = \n",
    "* 10\n",
    "* 1000\n",
    "\n",
    "Report the accuracies using bag of words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vugpX4XBHCzk"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjteJMM2HCzn"
   },
   "source": [
    "### Ungraded Exercise 2\n",
    "\n",
    "To classify news articles into their 20 news groups, experiment with the following parameter choices.\n",
    "\n",
    "* K-NN \n",
    " ** K : 10, 50\n",
    "\n",
    "Report the accuracies using bag of words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVmy22wSHCzo"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-f2KsA0nHCzr"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmNjJflBHCzs"
   },
   "source": [
    "Form the above experiment we can observe that the output of the bags of words would be a vector for each individual document. These documents will be parsed through different algorithms to extract the features that are used to classify the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQLiG9CF4OcV"
   },
   "source": [
    "### Please answer the questions below to complete the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgPPRD1sSGcr"
   },
   "outputs": [],
   "source": [
    "#@title There are two documnets D1 and D2. D1 = \" It was the best of times\". D2 = \"It was the worst of times\". What would be the IDF score of word \"It\"? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Answer = \"\" #@param [\"1\", \"0\", \"0.5\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "su7-3pD04KmG"
   },
   "outputs": [],
   "source": [
    "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Complexity = \" \" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWGaVOE24SDY"
   },
   "outputs": [],
   "source": [
    "#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
    "Additional = \" \" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXDbMn3b4Vld"
   },
   "outputs": [],
   "source": [
    "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Concepts = \"\" #@param [\"Yes\", \"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "6a23nTN34YyD"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
    "try:\n",
    "  if submission_id:\n",
    "      return_id = submit_notebook()\n",
    "      if return_id : submission_id =return_id\n",
    "  else:\n",
    "      print(\"Please complete the setup first.\")\n",
    "except NameError:\n",
    "  print (\"Please complete the setup first.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M1_001_BOW_20newsgroup_B.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
