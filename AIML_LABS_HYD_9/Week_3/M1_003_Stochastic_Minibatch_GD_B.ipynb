{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M1_003_Stochastic_Minibatch_GD_B.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"UERx5ALuTJW_","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"aE5xNmtpBcT4","colab_type":"text"},"cell_type":"markdown","source":["## Learning Objective:"]},{"metadata":{"id":"K_UItlExBcPR","colab_type":"text"},"cell_type":"markdown","source":["   \n","  At the end of the experiment, you will be able to :\n","    \n","  * Understand various types of gradient descent approaches (Stochastic, Mini-Batch Gradient Descent) and there differences.\n"]},{"metadata":{"id":"lo6h67YIkEKp","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Experiment Walkthrough\n","#@markdown Gradient descent and learning rate variations\n","from IPython.display import HTML\n","HTML(\"\"\"<video width='520' height='240' controls>\n","<source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Week_1/stochastic_minibatch_gradient_descent.mp4\" type='video/mp4'>\n","</video>\"\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2U7msw6GTJXB","colab_type":"text"},"cell_type":"markdown","source":["## Dataset"]},{"metadata":{"id":"PBplK_Hbw_EN","colab_type":"text"},"cell_type":"markdown","source":["### Description"]},{"metadata":{"id":"mDRTMSzJTJXD","colab_type":"text"},"cell_type":"markdown","source":["\n","The dataset consists of two columns and 89 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is  L∝T2 .\n"]},{"metadata":{"id":"Z8WpDaz2CukI","colab_type":"text"},"cell_type":"markdown","source":["##AI/ML Technique"]},{"metadata":{"id":"1jAMa55_xLmA","colab_type":"text"},"cell_type":"markdown","source":["#### Gradient Descent\n","\n","Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on a convex function, that tweaks it’s parameters iteratively to minimize a given function to its local minimum.\n","\n","To know more about Gradient Descent and its variants you can refer the below link:\n","\n","https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0"]},{"metadata":{"id":"zrwxriGkxNKp","colab_type":"text"},"cell_type":"markdown","source":["## Keywords"]},{"metadata":{"id":"mhOdzhI9TJXF","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","* Stochastic Gradient Descent\n","* Scipy\n","* Sklearn\n","* mini batch Gradient Descent\n","* Plotting Error vs Iteration"]},{"metadata":{"id":"s0Mvlz1fTJXF","colab_type":"text"},"cell_type":"markdown","source":["#### Expected Time : 90 mins"]},{"metadata":{"id":"fCtf1KWE40lx","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"98uM_xF8426S","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Km69XLfr5BGc","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7Xp91vRMpNo1","colab_type":"code","cellView":"both","colab":{}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook=\"M1W1_003_Stochastic_Minibatch_GD_B\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\") \n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Exp1/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", r[\"date\"])\n","      print(\"Time of submission: \", r[\"time\"])\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","    from IPython.display import HTML\n","    HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id))\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sRjAjk1hTJXM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import the required Packages\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as stat\n","%matplotlib notebook\n","from  matplotlib import pyplot as plt\n","import random\n","import time\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GLdAeF_tTJXR","colab_type":"text"},"cell_type":"markdown","source":["### Read the data"]},{"metadata":{"id":"YDNrlfakTJXS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load the data by using pandas read_csv()\n","data = pd.read_csv(\"AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n","# Print the first 5 rows of dataframe 'data'\n","print(data.head())\n","# Print the last 5 rows of dataframe 'data'\n","print(data.tail())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QTtgOro7TJXT","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get the length and time period values from the dataset\n","l = data['l'].values\n","t = data['t'].values\n","# Get the square of time period\n","tsq = t * t"],"execution_count":0,"outputs":[]},{"metadata":{"id":"srgyaBxoTJXW","colab_type":"text"},"cell_type":"markdown","source":["**Stochastic gradient descent (Single sample)**\n","\n","Instead of computing the sum of all gradients, stochastic gradient descent selects an observation uniformly at random."]},{"metadata":{"id":"hpHa2ZwyTJXX","colab_type":"text"},"cell_type":"markdown","source":[" $y_i = mx_i + c$\n","\n","$E$ = $(y - y_i)^2$\n","\n","$\\frac{\\partial E }{\\partial m}$ = $ -(y - (mx_i + c)) * x_i$\n","\n","$\\frac{\\partial E }{\\partial c}$ = $ -(y - (mx_i + c))$"]},{"metadata":{"id":"hUq_mCNLTJXY","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","The function 'next_step' updates the values of m and c and calculates error. \n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized loss is returned.\n","\"\"\"\n","def next_step(x, y, m, c, eta):\n","    ycalc = m * x + c\n","    error = (y - ycalc) ** 2\n","    delta_m = #<YOUR CODE HERE>\n","    delta_c = #<YOUR CODE HERE>\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    return m, c, error\n","\n","\"\"\"\n","The function below takes a random index and at that index idx, we calculate the values of m,c and error.\n","We use one data point at a time x[idx],y[idx]\n","Here we call the funtion 'next_step' to which we pass a data point x[idx],y[idx]\n","\"\"\"\n","def one_loop_random(x, y, m, c, eta):\n","    # Making random idx\n","    random_idx = np.arange(len(y))\n","    np.random.shuffle(random_idx)\n","    # Training with random idx\n","    for idx in random_idx:\n","        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n","        #print(m, c, e)\n","    return m,c,e\n","  \n","\"\"\"\n","The function below trains the data for 1000 iterations. \n","In each iteration it calls the 'one_loop_random' function.\n","\"\"\"\n","def train_stochastic(x, y, m, c, eta, iterations=1000):\n","    for iteration in range(iterations):\n","        m, c, err = one_loop_random(x, y, m, c, eta)\n","    return m, c, err"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HM2O8HtFTJXa","colab_type":"text"},"cell_type":"markdown","source":["## TRAIN"]},{"metadata":{"id":"JlR-YQJoTJXd","colab_type":"code","colab":{}},"cell_type":"code","source":["# Init m, c\n","m, c = 0, 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kijRetDpTJXg","colab_type":"code","colab":{}},"cell_type":"code","source":["# Learning rate\n","lr = 0.001"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8FDMh5YtTJXk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training for 1000 iterations, plotting after every 100 iterations:\n","%matplotlib inline\n","fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111)\n","plt.ion()\n","fig.show()\n","fig.canvas.draw()\n","\n","# Call the train_stochastic() method to update m and c and get error value with lr = 0.001.\n","for num in range(10):\n","    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100) # We will plot the error values for every 100 iterations\n","    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n","    y = m * l + c\n","    # use the axis \"ax\" defined above to plot, 'l vs tsq' and 'l vs y' i.e. use ax.plot\n","    #<YOUR CODE HERE>\n","    fig.canvas.draw()\n","    time.sleep(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HCVJrwLQTJXo","colab_type":"text"},"cell_type":"markdown","source":["**Ungraded Exercise 1: Experiment with other lr values.**\n","\n"]},{"metadata":{"id":"7vU_e7ilTJXo","colab_type":"code","colab":{}},"cell_type":"code","source":["##Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7GRP1Ge9TJXr","colab_type":"text"},"cell_type":"markdown","source":["**Ungraded Exercise 2: plot Errors vs Iterations**"]},{"metadata":{"id":"SXceKp3zTJXs","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your Code Here\n","ms, cs,errs = [], [], []\n","m, c = 0, 0\n","lr = 0.001\n","\n","# Call the train_stochastic() method to update m and c and get error value with lr = 0.001.\n","for times in range(100):\n","    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100) # We will plot the error values for every 100 iterations\n","    ms.append(m)\n","    cs.append(c)\n","    errs.append(error)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BDP56TXzywXx","colab_type":"code","colab":{}},"cell_type":"code","source":["### Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y9rIMmaMTJXv","colab_type":"text"},"cell_type":"markdown","source":["**Ungraded Exercise 3 : Is this better than sequential gradient descent and vanilla gradient descent?**\n","\n","Hint - check the error value at saturation, and time it takes to reach saturation."]},{"metadata":{"id":"_jhCKU75TJXv","colab_type":"code","colab":{}},"cell_type":"code","source":["#### Last Error at saturation: 0.004"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UwIZ7tuZTJXx","colab_type":"text"},"cell_type":"markdown","source":["## PROBLEM\n","\n","Problem with Sequential/Stochastic Gradient Descent is it does not scale well - it makes the same calculation of gradient descent on each sample. So the time taken will increase linearly with the number of samples. Many datasets have samples in the range of millions. Hence, even though it gives good results, it is not ideal.\n","\n","We need a gradient descent formulation that gives the speed of vanilla gradient descent and the accuracy of sequential/stochastic gradient descent.\n","\n","Next we will see **Minibatch Gradient Descent!**"]},{"metadata":{"id":"m3S-IvdHTJXx","colab_type":"text"},"cell_type":"markdown","source":["### Minibatch Gradient Descent"]},{"metadata":{"id":"ZT6lGuO8TJXy","colab_type":"text"},"cell_type":"markdown","source":["In Mini-Batch Gradient Descent algorithm, rather than using  the complete data set, in every iteration we use a subset of training examples (called \"batch\") to compute the gradient of the cost function. \n","\n","Common mini-batch sizes range between 50 and 256, but can vary for different applications."]},{"metadata":{"id":"8IWur68qTJXz","colab_type":"text"},"cell_type":"markdown","source":["one_batch() : we will be calculating the essenial parts of the Gradient Descent method:  \n","\n","$y_i = mx_i + c$\n","        \n","$E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y - y_i)^2$\n","\n","$\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y - (mx_i + c))$\n"," \n","$\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y - (mx_i + c))$"]},{"metadata":{"id":"mJgPCef6TJXz","colab_type":"text"},"cell_type":"markdown","source":["one_step() : We will be splitting our data into batches."]},{"metadata":{"id":"JkRMHxA2TJX0","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","The function 'train_one_batch' updates the values of m and c and calculates error. \n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized loss is returned.\n","\"\"\"\n","def train_one_batch(x, y, m, c, eta):\n","    const = - 2.0/len(y)\n","    ycalc = m * x + c\n","    delta_m = #<YOUR CODE HERE>\n","    delta_c = #<YOUR CODE HERE>\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    error = sum((y - ycalc)**2)/len(y)\n","    return m, c, error\n","\n","\"\"\"\n","The function below takes a batch_size and loss is calculated w.r.t batches.\n","The batches are created using random index.\n","The m, c and error values are calculated for each batch of data.\n","So, it calls the function 'train_one_batch' by passing batch_x, batch_y for each batch.\n","\"\"\"\n","def train_batches(x, y, m, c, eta, batch_size):\n","    # Making the batches\n","    random_idx = np.arange(len(y))\n","    np.random.shuffle(random_idx)\n","    \n","    # Train each batch\n","    for batch in range(len(y)//batch_size):\n","        batch_idx = random_idx[batch*batch_size:(batch+1)*batch_size]\n","        batch_x = x[batch_idx]\n","        batch_y = y[batch_idx]\n","        m, c, err = train_one_batch(batch_x, batch_y, m, c, eta)\n","    \n","    return m, c, err\n","\n","\"\"\"\n","The function below trains the data for 1000 iterations. \n","The data is traversed in batches, the batch size here is considered to be 10.\n","In each iteration it calls the 'train_batches' function. \n","The 'batch_size' is passed as a parameter to 'train_batches'.\n","\"\"\"\n","def train_minibatch(x, y, m, c, eta, batch_size=10, iterations=1000):\n","    for iteration in range(iterations):\n","        m, c, err = train_batches(x, y, m, c, eta, batch_size)\n","    return m, c, err\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-BLipYesTJX4","colab_type":"text"},"cell_type":"markdown","source":["#### TRAIN"]},{"metadata":{"id":"oAsrGHdLTJX4","colab_type":"code","colab":{}},"cell_type":"code","source":["# Init m, c\n","m, c = 0, 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BXm2ZSEwTJX8","colab_type":"code","colab":{}},"cell_type":"code","source":["# Learning rate\n","lr = 0.001"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c0ejZ17GTJX_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Batch size\n","batch_size = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Y-XhUFqTJYB","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","# Training for 1000 iterations, plotting after every 100 iterations:\n","fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111)\n","plt.ion()\n","fig.show()\n","fig.canvas.draw()\n","\n","# Call the train_minibatch() method to update m and c and get error value with lr = 0.001 and batch_size=90.\n","for num in range(10):\n","    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=90, iterations=100) # We will plot the error values for every 100 iterations\n","    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n","    y = m * l + c\n","    ax.clear()\n","    ax.plot(l, tsq, '.k')\n","    ax.plot(l, y)\n","    fig.canvas.draw()\n","    time.sleep(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bPrP3o7JTJYF","colab_type":"text"},"cell_type":"markdown","source":["**Ungraded Exercise 4: Experiment with other lr values.**"]},{"metadata":{"id":"zmQ_eNyQTJYG","colab_type":"code","colab":{}},"cell_type":"code","source":["## Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"stwbu3wYTJYI","colab_type":"text"},"cell_type":"markdown","source":["**Ungraded Exercise 5: Experiment with other batch_size values.**"]},{"metadata":{"id":"uBUcyH_QTJYJ","colab_type":"code","colab":{}},"cell_type":"code","source":["## Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vIFFYGKnTJYL","colab_type":"text"},"cell_type":"markdown","source":["\n","#### Plotting error vs iterations"]},{"metadata":{"id":"OhcEs6pVTJYL","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","\n","ms, cs,errs = [], [], []\n","m, c = 0, 0\n","lr = 0.001\n","batch_size = 10\n","\n","# Call the train_minibatch() method to update m and c and get error value with lr = 0.001 and batch_size = 10.\n","for times in range(100):\n","    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size, iterations=100) # We will plot the error values for every 100 iterations\n","    ms.append(m)\n","    cs.append(c)\n","    \n","    errs.append(error)\n","    \n","# plot Errors vs Iterations\n","epoch = range(0, 10000, 100)\n","plt.figure(figsize=(8, 5))\n","plt.plot(epoch, errs)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.title(\"Minibatch Gradient Descent\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rYuMICffTJYU","colab_type":"text"},"cell_type":"markdown","source":["**Ungraded Exercise 6: Is this better than sequential gradient descent and vanilla gradient descent?**\n","\n","Hint - check the error value at saturation, and time it takes to reach saturation."]},{"metadata":{"id":"sYqSG6SJTJYV","colab_type":"code","colab":{}},"cell_type":"code","source":["#### Last Error at saturation: 0.006"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7VKXYIXL5zCi","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"UYhUFOC2qDdb","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title For very large datasets, which of the following gradient descent methods is recommended? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"Stochastic\",\"Mini-batch\",\"Batch\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g9yfq4Cc5zqn","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E9zKHoZa52UE","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Gb5zgRl54z-","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Oiw7_YW958PG","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]}]}